{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR20GV2CAuhH"
      },
      "source": [
        "# Obtención de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZECYRJbHbi1V"
      },
      "source": [
        "## SIAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMfX2cDsgV_M"
      },
      "outputs": [],
      "source": [
        "# Importamos librerías\n",
        "import os\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import warnings\n",
        "import requests\n",
        "# !pip install lxml\n",
        "# !pip install pandas\n",
        "# !pip install beautifulsoup4\n",
        "# !pip install pyarrow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGpXj6vt6YyF"
      },
      "source": [
        "### Creamos catálogos con los identificadores para la API desde el JSON: Victor-dict.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKpjsfYoAuhl"
      },
      "outputs": [],
      "source": [
        "# Creamos el folder para los distintos catálogos (uno por filtro)\n",
        "if not os.path.exists('Catalogos_API_SIAP'):\n",
        "    os.mkdir(\"Catalogos_API_SIAP\")\n",
        "\n",
        "# Cargamos el json\n",
        "df = pd.read_json('Victor-dict.json',orient='index')\n",
        "\n",
        "# Creamos una lista con los nombres de los filtros y el nombre que tendrán los catálogos\n",
        "tag = [\"municipio\",\"anioagric\",\"cicloProd\",\"modalidad\",\"entidad\",\"cultivo\",\"distrito\",\"mesagric\"]\n",
        "name_csv = ['Municipio','Anio_Agricola','Ciclo_Prod','Modalidad','Entidad','Cultivo','Distrito',\"Mes_Agricola\"]\n",
        "\n",
        "# Iteramos sobre los filtros\n",
        "for i in range(8):\n",
        "    # Toma los datos pertenecientes a tag\n",
        "    df_aux2 = df.loc[df.index.str.startswith(tag[i])]\n",
        "    df_aux = df_aux2.copy()\n",
        "    # Elimina la str del tag+_ de la columna index\n",
        "    df_aux.index = df_aux.index.str.replace(tag[i]+'_', \"\")\n",
        "    # Convierte el indice en columna\n",
        "    df_aux.reset_index(inplace=True)\n",
        "    # Cambia los nombres de las columnas\n",
        "    df_aux.rename(columns={0: \"ID\",\"index\":\"Name\"}, inplace=True)\n",
        "    # Crea el path donde se guardara\n",
        "    ruta_completa = os.path.join(\"Catalogos_API_SIAP\",name_csv[i])\n",
        "    # Guardar el DataFrame en el archivo CSV en la carpeta \"Catalogos_API_SIAP\"\n",
        "    df_aux.to_csv(ruta_completa + '.csv', index=False)\n",
        "\n",
        "#Elimina los df\n",
        "del df_aux, df_aux2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiFFWRv7Auhw"
      },
      "source": [
        "### Descarga de de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRsE3clfAuhx"
      },
      "outputs": [],
      "source": [
        "# Creacion del folder Raw\n",
        "if not os.path.exists('Raw'):\n",
        "    os.mkdir(\"Raw\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9ie4VAdAuhx"
      },
      "outputs": [],
      "source": [
        "# Comando para ignorar warnings (salen en cada request)\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yequPVhdAuhy"
      },
      "outputs": [],
      "source": [
        "# Creamos iterables para años, meses y cultivos\n",
        "anio_arr = pd.read_csv('./Catalogos_API_SIAP/Anio_Agricola.csv').ID.to_list()\n",
        "cultivo_arr = pd.read_csv('./Catalogos_API_SIAP/Cultivo.csv').ID.to_list()\n",
        "mes_arr = pd.read_csv('./Catalogos_API_SIAP/Mes_Agricola.csv').ID.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA-d0oo-Auhy"
      },
      "outputs": [],
      "source": [
        "# En el iterable de cultibos eliminamos la opción 0\n",
        "cultivo_arr.remove(0) # 0: Todos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mu5l6SybaXv"
      },
      "outputs": [],
      "source": [
        "# En Raw creamos una carpeta para cada año\n",
        "for i in anio_arr:\n",
        "    if not os.path.exists('Raw/'+str(i)):\n",
        "        os.mkdir(\"Raw/\"+str(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDw4TsGQAuh0"
      },
      "outputs": [],
      "source": [
        "# Función para hacer un request a la API\n",
        "def API_SIAP(anio,mes,cultivo):\n",
        "  \"\"\"Esta función permite hacer un request a la API de avance agrícola mensual y recibe como argumento el año,\n",
        "     mes y cultivo del que queremos obtener la información. Los demás filtros reciben un valor fijo.\"\"\"\n",
        "\n",
        "  # Endpoint\n",
        "  url = \"https://nube.siap.gob.mx/avance_agricola/\"\n",
        "\n",
        "  # Diccionario con atributos del request\n",
        "  payload = {'xajax': 'reporte', # Para obtener la tabla\n",
        "  'xajaxr': '1696449941927', # Timestamp UNIX\n",
        "  'xajaxargs[]': [\n",
        "      '1', # 1: Desglose por estados, 2: Cultivo total\n",
        "      str(anio), # Anio\n",
        "      '5', # ID Ciclo\n",
        "      '3', # ID Modalidad\n",
        "      '0', # ID Estado (0: Nacional)\n",
        "      '--',\n",
        "      '--',\n",
        "      str(cultivo), # ID Cultivo\n",
        "      '200201',\n",
        "      '0',\n",
        "      '1', # 1: Por municipio\n",
        "      'undefined',\n",
        "      'undefined',\n",
        "      'undefined',\n",
        "      str(mes) # Valor del mes: va de 1 a 8, con 1 siendo Enero y 8 Agosto\n",
        "      ]\n",
        "  }\n",
        "\n",
        "  headers = {\n",
        "    'Cookie': 'PHPSESSID=45ri2k73cbp2iptcrufu88p360'\n",
        "  }\n",
        "\n",
        "  # Hacemos el request\n",
        "  response = requests.request(\"POST\", url, headers=headers, data=payload, verify=False)\n",
        "  response.encoding='ISO-8859-1'\n",
        "\n",
        "  # Regresamos un string con el XML con la tabla que contiene los datos solicitados\n",
        "  return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B-UPvA2FHfa"
      },
      "outputs": [],
      "source": [
        "# Iteramos anidadamente sobre los años, meses y cultivos de nuestro interés\n",
        "\n",
        "for anio in anio_arr:     # Iteramos en los años\n",
        "    for mes in mes_arr:   # Iteramos en los meses\n",
        "\n",
        "        # Lista con el nombre de nuestras columnas\n",
        "        columnas = [\"Entidad\", \"Municipio\", \"Superficie Sembrada\", \"Superficie Cosechada\", \"Superficie Siniestrada\", \"Produccion\", \"Rendimiento\",\"Anio\", \"Mes\", \"Cultivo\"]\n",
        "        df_raw = pd.DataFrame(columns=columnas)  # Creamos un DataFrame vacío con estas columnas\n",
        "\n",
        "        for cultivo in cultivo_arr:                 # Iteramos en los cultivos\n",
        "            xml_data = API_SIAP(anio,mes,cultivo)   # Hcemos el API request\n",
        "            soup = BeautifulSoup(xml_data, 'lxml')  # Parseamos el XML con BeautifulSoup usando el analizador XML (lxml)\n",
        "\n",
        "            # Encontramos la tabla por su ID\n",
        "            tabla = soup.find('table', id='Resultados-reporte')\n",
        "\n",
        "            if tabla:\n",
        "                # Inicializamos listas para almacenar los datos\n",
        "                datos = []\n",
        "\n",
        "                # Iteramos a través de las filas de la tabla\n",
        "                for fila in tabla.find_all('tr')[1:]:  # Ignoramos la primera fila de encabezados\n",
        "                    columnas = fila.find_all('td')     # Identificamos todas las celdas\n",
        "                    if len(columnas) == 0:             # Ignoramos las filas vacías\n",
        "                        continue\n",
        "\n",
        "                    # Extraemos los datos relevantes de cada columna\n",
        "                    #posicion = columnas[0].get_text()\n",
        "                    entidad = columnas[1].get_text()\n",
        "                    municipio = columnas[2].get_text()\n",
        "                    superficie_sembrada = columnas[3].get_text()\n",
        "                    superficie_cosechada = columnas[4].get_text()\n",
        "                    superficie_siniestrada = columnas[5].get_text()\n",
        "                    produccion = columnas[6].get_text()\n",
        "                    rendimiento = columnas[7].get_text()\n",
        "\n",
        "                    # Agregamos los datos a la lista\n",
        "                    datos = [\n",
        "                        entidad, municipio,\n",
        "                        superficie_sembrada, superficie_cosechada,\n",
        "                        superficie_siniestrada, produccion, rendimiento,\n",
        "                        anio, mes, cultivo\n",
        "                    ]\n",
        "\n",
        "                    # Agregamos estos datos al DataFrame\n",
        "                    df_raw.loc[len(df_raw)] = datos\n",
        "\n",
        "        # Guardamos un CSV cuyo nombre sigue el formato: \"Avance_agricola_año_mes.csv\"\n",
        "        name_csv = 'Avance_Agricola_'+str(anio)+'_'+str(mes)\n",
        "        df_raw.to_csv('./Raw/'+str(anio)+'/'+name_csv+'.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8yRDiR0gDHG"
      },
      "source": [
        "## SNIIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQO4qVNsgYPq"
      },
      "outputs": [],
      "source": [
        "# Importamos librerías\n",
        "import datetime\n",
        "from distutils.command.build_scripts import first_line_re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# from sniim.db.mongo import Mongoclient\n",
        "from clint.textui import puts, colored, indent\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzivuSd6gEy8"
      },
      "outputs": [],
      "source": [
        "# Creamos un objeto para hacer el scrapp al SNIIM\n",
        "class ScrapperMarketAgriculture:\n",
        "\n",
        "    # Variables de clase para rastrear estadísticas\n",
        "    total_records = 0\n",
        "    inserted_records = 0\n",
        "    current_product = 'None'\n",
        "    first_print = True\n",
        "\n",
        "    # URL base para acceder al SNIIM\n",
        "    base_url = 'http://www.economia-sniim.gob.mx/NUEVO/Consultas/MercadosNacionales/PreciosDeMercado/Agricolas'\n",
        "\n",
        "    # URLs iniciales para diferentes categorías de productos\n",
        "    init_urls = [\n",
        "        ['Frutas y Hortalizas', '/ConsultaFrutasYHortalizas.aspx', '/ResultadosConsultaFechaFrutasYHortalizas.aspx'],\n",
        "        ['Flores', '/ConsultaFlores.aspx?SubOpcion=5', '/ResultadosConsultaFechaFlores.aspx'],\n",
        "        ['Granos', '/ConsultaGranos.aspx?SubOpcion=6', '/ResultadosConsultaFechaGranos.aspx'],\n",
        "        ['Aceites', '/ConsultaAceites.aspx?SubOpcion=8', '/ResultadosConsultaFechaAceites.aspx']\n",
        "    ]\n",
        "\n",
        "    # Lista de productos ya obtenidos: permite optimizar cuando ya se han descargado ciertos cultivos\n",
        "    # saved_products = [\n",
        "    #     'acelga_primera',\n",
        "    #     'aguacate_criollo_primera',\n",
        "    #     'aguacate_fuerte_primera',\n",
        "    #     'aguacate_hass_adelantado_primera',\n",
        "    #     'aguacate_hass_calidad_super_extra',\n",
        "    #     'aguacate_hass_flor_vieja_primera',\n",
        "    #     'aguacate_hass_primera',\n",
        "    #     'aguacate_pagua_primera',\n",
        "    #     'ajo_blanco_#_8_primera',\n",
        "    #     'ajo_blanco_primera',\n",
        "    #     'ajo_morado_primera',\n",
        "    #     'apio_primera',\n",
        "    #     'berenjena_primera',\n",
        "    #     'betabel_primera'\n",
        "    # ]\n",
        "\n",
        "    # Último año de datos disponibles\n",
        "    last_year = 2022\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # Constructor, se puede pasar un argumento 'is_historic' como booleano\n",
        "        self.is_historic = kwargs.get('is_historic', True)\n",
        "        self.df = pd.DataFrame()  # Inicializa un DataFrame para almacenar los datos\n",
        "\n",
        "    def read_category(self, category, url, url_form):\n",
        "        # Función para leer los productos de una categoría\n",
        "        category_page = requests.get(self.base_url + url)\n",
        "        category_page = BeautifulSoup(category_page.content, features=\"html.parser\")\n",
        "\n",
        "        # Obtiene la lista de productos disponibles\n",
        "        products = [(product.getText(), product['value'], ) for product in category_page.select_one('select#ddlProducto').find_all('option')]\n",
        "\n",
        "        for product in products:\n",
        "            product_name, product_id = product\n",
        "            if product_id == '-1':\n",
        "                continue\n",
        "\n",
        "            # Si ya se está recopilando información de un producto anterior, guárdalo en un archivo CSV\n",
        "            if self.current_product != 'None':\n",
        "              self.df.to_csv(f'sniim_product_{self.current_product}.csv', index=False)\n",
        "            self.current_product = str(product_name).lower().replace('-', '').replace('  ', '_')\n",
        "\n",
        "            # Si el producto ya se ha obtenido previamente, omítelo\n",
        "            #if self.current_product in self.saved_products:\n",
        "            #  continue\n",
        "\n",
        "            self.first_print = True\n",
        "            self.df = pd.DataFrame()  # Reinicializa el DataFrame para el nuevo producto\n",
        "\n",
        "            with indent(4):\n",
        "                puts(colored.magenta(\"Producto: {}\".format(self.current_product)))\n",
        "\n",
        "            if self.is_historic:\n",
        "                # Si se desea obtener datos históricos\n",
        "                for year in range(2018, 2024):\n",
        "                    payload = {\n",
        "                        'fechaInicio':'01/01/{0}'.format(str(year)),\n",
        "                        'fechaFinal':'01/01/{0}'.format(str(year + 1)),\n",
        "                        'ProductoId':product_id,\n",
        "                        'OrigenId':'-1',\n",
        "                        'Origen':'Todos',\n",
        "                        'DestinoId':'-1',\n",
        "                        'Destino':'Todos',\n",
        "                        'PreciosPorId':'2',\n",
        "                        'RegistrosPorPagina':'1000'\n",
        "                    }\n",
        "\n",
        "                    if not self.gather_prices(payload, url_form):\n",
        "                        continue\n",
        "            else:\n",
        "                # Si se desea obtener datos en tiempo real\n",
        "                today = datetime.datetime.today()\n",
        "                deleta = datetime.timedelta(days=-1)\n",
        "                payload = {\n",
        "                        'fechaInicio':'{}'.format(today.strftime('%d/%m/%Y')),\n",
        "                        'fechaFinal':'{}'.format((today).strftime('%d/%m/%Y')),\n",
        "                        'ProductoId':product_id,\n",
        "                        'OrigenId':'-1',\n",
        "                        'Origen':'Todos',\n",
        "                        'DestinoId':'-1',\n",
        "                        'Destino':'Todos',\n",
        "                        'PreciosPorId':'2',\n",
        "                        'RegistrosPorPagina':'1000'\n",
        "                    }\n",
        "\n",
        "                if not self.gather_prices(payload, url_form):\n",
        "                    continue\n",
        "\n",
        "        return\n",
        "\n",
        "    def scraping(self):\n",
        "        # Función principal para iniciar el proceso de obtención de datos\n",
        "        self.total_records = 0\n",
        "        self.inserted_records = 0\n",
        "\n",
        "        for category, url, url_form in self.init_urls:\n",
        "            self.read_category(category, url, url_form)\n",
        "            time.sleep(60)\n",
        "\n",
        "    def gather_prices(self, payload, url_form):\n",
        "        # Función para obtener los precios de los productos\n",
        "        with indent(4):\n",
        "            puts(colored.blue(\"Petición: {}\".format(str(payload)))\n",
        "        # Hacemos el request\n",
        "        response = requests.get(self.base_url + url_form, params=payload)\n",
        "        time.sleep(30)  # Espera 30 segundos entre peticiones para evitar sobrecargar el servidor\n",
        "        # Evaluamos que el request haya sido exitosa\n",
        "        if response.status_code != 200:\n",
        "            with indent(4):\n",
        "                puts(colored.red(\"Error en la petición HTTP: {}\".format(str(response.text)))\n",
        "            return False\n",
        "        # Parseamos el HTML con la tabla que contiene los datos\n",
        "        product_prices = BeautifulSoup(response.content, features=\"html.parser\")\n",
        "\n",
        "        try:\n",
        "            table_prices = product_prices.select_one('table#tblResultados')  # Busca la tabla de precios en el HTML\n",
        "        except Exception as error:  # Catchamos el error en caso de que no haya resultados\n",
        "            with indent(4):\n",
        "                puts(colored.red(\"Error en el análisis del HTML: {}\".format(str(error)))\n",
        "            return False\n",
        "\n",
        "        # Nombre que tendrán las columnas de los datos\n",
        "        fields = ('fecha', 'presentacion', 'origen', 'destino', 'precio_min', 'precio_max', 'precio_frec', 'obs')\n",
        "        counter_row = 0\n",
        "\n",
        "        for observation in table_prices.find_all('tr'):  # Iteramos sobre las rows\n",
        "            if counter_row > 1:  # Ignoramos encabezados\n",
        "                row = {}\n",
        "                counter_field = 0\n",
        "                if self.first_print:\n",
        "                  self.first_print = False  # Variable para imprimir encabezados solo una vez\n",
        "                for metric in observation.find_all('td'):  # Iteramos sobre columnas\n",
        "                    row[fields[counter_field]] = metric.getText()  # Obtenemos los datos en cada celda\n",
        "                    counter_field += 1  # Incrementamos contador de columnas\n",
        "\n",
        "                row['name'] = self.current_product   # Agregar el nombre del producto al registro\n",
        "                df2 = pd.DataFrame(row, index=[0])\n",
        "                self.df = pd.concat([self.df, df2])  # Agregar la fila a un DataFrame existente\n",
        "\n",
        "            self.total_records += 1  # Incrementamos el contador de instancias\n",
        "            counter_row += 1         # Incrementamos el contador de filas\n",
        "\n",
        "            # El limite de registros es 1000\n",
        "            if self.total_records % 1000 == 0:\n",
        "              # Sí se alcanza el límite creamos el CSV\n",
        "              self.df.to_csv(f'sniim_product_{self.current_product}.csv', index=False)\n",
        "\n",
        "        return True\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # Ejecutar el scraper para obtener los datos\n",
        "  agricola = ScrapperMarketAgriculture()\n",
        "  agricola.scraping()\n",
        "\n",
        "#     vacas = ScrapperMarketLiveStock()\n",
        "#     vacas.scraping()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
